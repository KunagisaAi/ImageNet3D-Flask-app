{% extends "layout.html" %}

{% block title %}
<title>ImageNet3D - Annotate</title>
{% endblock %}

{% block content %}
    <div class="container pt-4">

        <h1 class="pb-2">Documentation</h1>

        <div class="col-12 px-0" style="margin-bottom: 48px;">

            <h3 class="pb-2">Overview</h3>

            <p>
                In the previous assignment we looked into synthetic images generated by diffusion models that come with pseudo 3D annotations<sup><a href="#ref1">[1]</a></sup>. We found that about 75% of the synthetic images are consistent with the pseudo 3D annotations while the other 25% have inconsistent viewpoint or shape. Although as a synthetic dataset it is a little noisy, we have shown that NMMs<sup><a href="#ref2">[2]</a></sup> can effectively learn from the "good" samples and boost performance on both in-distribution and out-of-distribution data.
            </p>

            <p>
                This opens many new opportunities for 3D understanding (e.g., grounding, detection, segmentation) -- models can be built on knowledge learned by existing LLMs and generative diffusion models. However, to analyze the performance of such models, we still need to develop a dataset with accurate 3D annotations to evaluate the zero-shot/few-shot performance of such models.
            </p>

            <p>
                <b>How do we describe an object in the 3D space?</b> In this project, we specify the 3D viewpoint of the object (see image below), the 2D location of the object (in the image plane), and the distance from the camera to the object. These six parameters allows us to fully specify the 3D location and 3D rotation of the object. Moreover, we are also interested in which subtype of the object is in the image -- if it is a car, is it a sedan or a hatchback. This is accomplished by associating the object with a best matching mesh model from a list of models given (you will see all the mesh models available in the bottom right corner of the annotation page).
            </p>

            <p>
                Lastly every real image comes with an existing annotation obtained from a pretrained 3D model. Your job would be refining the prediction of the model, as well as labeling the quality of the object. See below for a full list of "things to do" for each real image.
            </p>

            <p>
                <b><span style="color: red">Read me &#8592; </span> Before you start, it is very important for you to go over this <a href="https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view?usp=sharing">tutorial</a> with qualitative examples demonstrating many important details. The first part of the tutorial is similar to this documentation and the second part of the tutorial provides a lot of qualitative examples. Make sure you go over a number of the examples in part two to make sure you fully grasp the idea.</b>
            </p>

            <img src="{{ url_for('static', filename='images/imagenet3d_viewpoint.png') }}" class="img-fluid pb-2" style="max-width: 50%">

            <h3 class="pb-2 pt-4">Uesr Interface</h3>

            <p>Log in to the web app with your annotator ID. You will see the list of tasks assigned to you and track your progress in this page.</p>

            <img src="{{ url_for('static', filename='images/ui_login.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <p>
                Clicking on a task will redirect you to the first unannotated question. On the top you will see basic information about this question, as well as multiple control buttons. Hovering on the buttons will show you what they do.
                <ul>
                    <li>Move between questions with "<", ">", or ">>".</li>
                    <li>Delete current saved/unsaved annotation with the yellow reverse button.</li>
                    <li>Save the current annotaion with the green save button. (Or hit "Enter" as shortcut.)</li>
                </ul>
            </p>

            <img src="{{ url_for('static', filename='images/ui_annotate.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <p>The list of different CAD models are visualized on the bottom. Besides the 6D pose and type of model, you will also answer two questions on object quality and scene density.</p>

            <img src="{{ url_for('static', filename='images/ui_cads.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <p>After labeling all information, clicking the green save button or pushing "Enter" key to save the annotation. If successful, you will see a message and the question status will be lighted.</p>

            <img src="{{ url_for('static', filename='images/ui_save.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <h3 class="pb-2 pt-4">Guidelines</h3>

            <p>
                <b>Matching mesh model (with initializations provided by a pretrained model).</b> The best matching model from a list of mesh models given. This is crucial to make accurate estimations of the following parameters so this should be the first thing to do. Click on "<- Model" and "Model ->" buttons to change the mesh model selection.
            </p>

            <p>
                <b>3D rotation (with initializations provided by a pretrained model).</b> Three parameters are used to specify the 3D rotation of an object: azimuth, elevation, and in-plane rotation (theta). Adjust the three parameters so the 3D rotation of the rendered object aligns with the 3D rotation of the object in the image. Make sure you are not simply aligning the segmentation or boundaries of the two objects. You should focus on aligning the 3D rotation of the objects so the rendered object is "pointing" to the same direction as the object in the image.
            </p>

            <p>
                <b>2D location (with initializations provided by a pretrained model).</b> 2D location specifies the location of the center of the object.
            </p>

            <p>
                <b>Distance (with initializations provided by a pretrained model).</b> Distance between the object and the camera. To annotate the distance, make sure the "size" of the rendered object is roughly the same as the "size" of the object in the image.
            </p>

            <p>
                <b>Object quality.</b> Object quality specifies how clearly the object is visible from the image. A "good" object would be clearly visible and not occluded. A "bad" object may be barely visible (imaging drving in heavy fog when cars in front of you are barely visible) or occluded by other objects. Several choices are considered:
                <ul>
                    <li><i>Good.</i> Most part (more than 90%) of the object is clearly visible in the image.</li>
                    <li><i>Partially visible.</i> A small part of the object is occluded by other objects or outside the image (truncated by image boundary).</li>
                    <li><i>Barely visible.</i> only a small part of the object is clearly visible -- the other parts are either occluded or outside the image, or barely visible due to other reasons (e.g., weather).</li>
                    <li><i>Bad quality / no object.</i> Most part of the object is occluded or outside the image; or the pose of the object is very hard to tell.</li>
                </ul>
            </p>

            <p>
                <b>Dense scene.</b> This parameter tells if an object is very close to another object from the same category. Here "close" is defined in the 2D image plane -- two objects are close if the distance between them is small in the 2D image plane.
                <ul>
                    <li><i>Not dense scene.</i> The object is not close to another object from the same category. There can be multiple objects from the same category in one image but the objects are far away from each other.
                    <li><i>Dense scene.</i> The object is very close to another object from the same category. They may occlude each other or simply very close to each other.</li>
                </ul>
            </p>

            <h3 class="pb-2 pt-4">Examples</h3>

            <p>See <a href="https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view?usp=sharing">tutorial</a>.</p>

            <h2 class="pb-2">References</h2>

            <p id="ref1">
                [1] <a href="https://arxiv.org/abs/2306.08103">Adding 3D Geometry Control to Diffusion Models</a>
            </p>

            <p id="ref2">
                [2] <a href="https://arxiv.org/abs/2209.05624">Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features</a>
            </p>

        </div>

    </div>
{% endblock %}
