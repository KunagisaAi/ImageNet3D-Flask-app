{% extends "layout.html" %}

{% block title %}
<title>ImageNet3D - Annotate</title>
{% endblock %}

{% block content %}
    <div class="container pt-4">

        <h1 class="pb-2">Documentation</h1>

        <div class="col-12 px-0" style="margin-bottom: 48px;">

            <h3 class="pb-2">Overview</h3>

            <p>
                In the previous assignment we looked into synthetic images generated by diffusion models that come with pseudo 3D annotations<sup><a href="#ref1">[1]</a></sup>. We found that about 75% of the synthetic images are consistent with the pseudo 3D annotations while the other 25% have inconsistent viewpoint or shape. Although as a synthetic dataset it is a little noisy, we have shown that NMMs<sup><a href="#ref2">[2]</a></sup> can effectively learn from the "good" samples and boost performance on both in-distribution and out-of-distribution data.
            </p>

            <p>
                This opens many new opportunities for 3D understanding (e.g., grounding, detection, segmentation) -- models can be built on knowledge learned by existing LLMs and generative diffusion models. However, to analyze the performance of such models, we still need to develop a dataset with accurate 3D annotations to evaluate the zero-shot/few-shot performance of such models.
            </p>

            <p>
                How do we describe an object in the 3D space? In this project, we specify the 3D viewpoint of the object (see image below), the 2D location of the object, and the distance from the camera to the object. This allows us to fully specify the 3D location and rotation of the object. Moreover, we are also interested in which subtype of the object is in the image -- if it is a car, is it a sedan or a hatchback. This is accomplished by associating the object with a best matching mesh model from a list of models given (you will see all the mesh models available in the bottom right corner of the annotation page).
            </p>

            <p>
                Lastly every real image comes with an existing annotation obtained from a pretrained 3D model. Your job would be refining the prediction of the model, as well as labeling the quality of the object. See below for a full list of "things to do" for each real image.
            </p>

            <img src="{{ url_for('static', filename='images/imagenet3d_viewpoint.png') }}" class="img-fluid pb-2" style="max-width: 50%">

            <h3 class="pb-2 pt-4">Uesr Interface</h3>

            <p><b>Step 1.</b> Log in to the web app with your annotator ID.</p>

            <img src="{{ url_for('static', filename='images/imagenet3d_login.png') }}" class="img-fluid" style="max-width: 50%">

            <p><b>Step 2.</b> After logged in, you will be redirected to the account page where you will see the lists of tasks you are assigned to and the current annotation progress.</p>

            <img src="{{ url_for('static', filename='images/imagenet3d_account.png') }}" class="img-fluid" style="max-width: 50%">

            <p><b>Step 3.</b> Start annotating the data by clicking on any of the task names. Hovering on buttons or icons without texts will reveal text explanations.</p>

            <img src="{{ url_for('static', filename='images/imagenet3d_annotate.png') }}" class="img-fluid" style="max-width: 50%">

            <h3 class="pb-2 pt-4">Guidelines</h3>

            <ul>

                <li><b>3D rotation (with initializations provided by a pretrained model).</b> Three parameters are used to specify the 3D rotation of an object: azimuth, elevation, and in-plane rotation. Adjust the three parameters so the 3D rotation of the rendered object aligns with the 3D rotation of the object in the image. Make sure you are not simply aligning the segmentation or boundaries of the two objects. You should focus on aligning the 3D rotation of the objects so the rendered object is "pointing" to the same direction as the object in the image.</li>

                <li><b>2D location (with initializations provided by a pretrained model).</b>2D location specifies the location of the center of the object.</li>

                <li><b>Distance (with initializations provided by a pretrained model).</b> Distance between the object and the camera. To annotate the distance, make sure the "size" of the rendered object is roughly the same as the "size" of the object in the image.</li>

                <li><b>Matching mesh model (with initializations provided by a pretrained model).</b> The best matching model from a list of mesh models given.</li>

                <li><b>Object quality.</b> Object quality specifies how clearly the object is visible from the image. Several choices are considered</li>

                <ul>
                    <li><i>Good.</i> Most part (more than 90%) of the object is clearly visible in the image.</li>
                    <li><i>Partially visible.</i> A small part of the object is occluded by other objects or outside the image (truncated by image boundary).</li>
                    <li><i>Barely visible.</i> only a small part of the object is clearly visible -- the other parts are either occluded or outside the image.</li>
                    <li><i>Bad quality / no object.</i> Most part of the object is occluded or outside the image; or the pose of the object is hard to tell.</li>
                </ul>

                <li><b>Dense scene.</b> The parameter tells if an object is very close to another object from the same category. Here "close" is defined in the 2D image plane -- two objects are close if the distance between them is small in the 2D image plane.</li>

                <ul>
                    <li><i>Not dense scene.</i> The object is not close to another object from the same category. There can be multiple objects from the same category in one image but the objects are far away from each other.
                    <li><i>Dense scene.</i> The object is very close to another object from the same category. They may occlude each other or simply very close to each other.</li>
                </ul>

            </ul>

            <h2 class="pb-2">References</h2>

            <p id="ref1">
                [1] <a href="https://arxiv.org/abs/2306.08103">Adding 3D Geometry Control to Diffusion Models</a>
            </p>

            <p id="ref2">
                [2] <a href="https://arxiv.org/abs/2209.05624">Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features</a>
            </p>

        </div>

    </div>
{% endblock %}
