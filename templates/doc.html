{% extends "layout.html" %}

{% block title %}
<title>ImageNet3D - Annotate</title>
{% endblock %}

{% block content %}
    <div class="container pt-4">

        <h1 class="pb-2">Documentation</h1>

        <div class="col-12 px-0" style="margin-bottom: 48px;">

            <h3 class="pb-2">Overview</h3>

            <p>
                In the previous assignment we looked into synthetic images generated by diffusion models that come with pseudo 3D annotations<sup><a href="#ref1">[1]</a></sup>. We found that about 75% of the synthetic images are consistent with the pseudo 3D annotations while the other 25% have inconsistent viewpoint or shape. Although as a synthetic dataset it is a little noisy, we have shown that NMMs<sup><a href="#ref2">[2]</a></sup> can effectively learn from the "good" samples and boost performance on both in-distribution and out-of-distribution data.
            </p>

            <p>
                This opens many new opportunities for 3D understanding (e.g., grounding, detection, segmentation) -- models can be built on knowledge learned by existing LLMs and generative diffusion models. However, to analyze the performance of such models, we still need to develop a dataset with accurate 3D annotations to evaluate the zero-shot/few-shot performance of such models.
            </p>

            <p>
                <b>How do we describe an object in the 3D space?</b> In this project, we specify the 3D viewpoint of the object (see image below), the 2D location of the object (in the image plane), and the distance from the camera to the object. These six parameters allows us to fully specify the 3D location and 3D rotation of the object. Moreover, we are also interested in which subtype of the object is in the image -- if it is a car, is it a sedan or a hatchback. This is accomplished by associating the object with a best matching mesh model from a list of models given (you will see all the mesh models available in the bottom right corner of the annotation page).
            </p>

            <p>
                Lastly every real image comes with an existing annotation obtained from a pretrained 3D model. Your job would be refining the prediction of the model, as well as labeling the quality of the object. See below for a full list of "things to do" for each real image.
            </p>

            <p>
                <b><span style="color: red">Read me &#9786; &#8594; </span> Before you start, it is very important for you to go over this <a href="https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view?usp=sharing">tutorial</a> with qualitative examples demonstrating many important details. The first part of the tutorial is similar to this documentation and the second part of the tutorial provides a lot of qualitative images. Please go over a number of the examples in part two to make sure you fully grasp the idea.</b>
            </p>

            <img src="{{ url_for('static', filename='images/imagenet3d_viewpoint.png') }}" class="img-fluid pb-2" style="max-width: 50%">

            <h3 class="pb-2 pt-4">New for v2 (Mar 2024)</h3>

            <ol>
                <li>As we start to deal with more challenging classes, we no longer have good initializations for the 3D orientation. New buttons are added with "↓↓" and "↑↑", which mean they are coarse buttons with bigger steps.</li>
                <li>Since we don't have good initializations, labeling one image may takes longer. Please be patient and make sure the results are visually correct.</li>
                <li>The quality of the images can be lower too -- some bounding boxes may not correspond to the class of interest. It is therefore important to label and disregard "bad" objects. Label the question as "bad quality/no object" so we will disregard this sample. In this case, you may skip tuning other parameters.</li>
                <li>Similar as before, it is important to label the visibility and the scene density. If you have gone through the <a href="https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view?usp=sharing">tutorial</a>, you should have a good feeling of how to label these questions.</li>
            </ol>

            <p>Some other class-specific notes:</p>

            <ul>
                <li><b>bicycle-built-for-two:</b> We don't have good mesh models for tandem bicycles so we will be using mesh models of normal bicycles. This shouldn't be a problem as long as you match the 2D center and 3D orientation of the tandem bicycle in the image and the rendered mesh model.</li>
                <li><b>beaker:</b> There are different subtypes of beakers but we don't have good mesh models covering every shape. This wouldn't be a problem for the pose parameters we consider here. For 3D viewpoint, match the orientation of the beaker notch. 2D location and distance should be straightforward too.</li>
                <li><b>oboe:</b> It's almost impossible to tell the "azimuth" of an oboe. Just ignore this parameter and match the others.</li>
                <li><b>crutch:</b> There can be different types of crutches (see <a href="https://www.physio-pedia.com/File:Types_of_crutches.jpg">here</a>). We only want the type that matches the mesh model we have (i.e., axillary crutch). For other types of crutches, just label them as "bad quality/no object".</li>
                <li><b>punching_bag:</b> There are different types of punching bags (see <a href="https://img2.storyblok.com/1800x743/filters:focal(null):format(webp)/f/115220/2400x990/eae71daccc/how-to-choose-the-right-punching-bag-for-your-workout.jpeg">here</a>). We only want boxing bag and hanging bags, with a cylinder shape. For punching bags, also ignore the "azimuth" parameter.</li>
            </ul>

            <h3 class="pb-2 pt-4">Uesr Interface</h3>

            <p>Log in to the web app with your annotator ID. You will see the list of tasks assigned to you and track your progress in this page.</p>

            <img src="{{ url_for('static', filename='images/ui_login.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <p>
                Clicking on a task will redirect you to the first unannotated question. On the top you will see basic information about this question, as well as multiple control buttons. Hovering on the buttons will show you what they do.
                <ul>
                    <li>Move between questions with "<", ">", or ">>".</li>
                    <li>Delete current saved/unsaved annotation with the yellow reverse button.</li>
                    <li>Save the current annotaion with the green save button. (Or hit "Enter" as shortcut.)</li>
                </ul>
            </p>

            <img src="{{ url_for('static', filename='images/ui_annotate.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <p>The list of different CAD models are visualized on the bottom. Besides the 6D pose and type of model, you will also answer two questions on object quality and scene density.</p>

            <img src="{{ url_for('static', filename='images/ui_cads.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <p>After labeling all information, clicking the green save button or pushing "Enter" key to save the annotation. If successful, you will see a message and the question status will be lighted.</p>

            <img src="{{ url_for('static', filename='images/ui_save.png') }}" class="img-fluid" style="max-width: 50%; padding-bottom: 20px;">

            <h3 class="pb-2 pt-4">Guidelines</h3>

            <p>
                <b>Matching mesh model (with initializations provided by a pretrained model).</b> The best matching model from a list of mesh models given. This is crucial to make accurate estimations of the following parameters so this should be the first thing to do. Click on "<- Model" and "Model ->" buttons to change the mesh model selection.
            </p>

            <p>
                <b>3D rotation (with initializations provided by a pretrained model).</b> Three parameters are used to specify the 3D rotation of an object: azimuth, elevation, and in-plane rotation (theta). Adjust the three parameters so the 3D rotation of the rendered object aligns with the 3D rotation of the object in the image. Make sure you are not simply aligning the segmentation or boundaries of the two objects. You should focus on aligning the 3D rotation of the objects so the rendered object is "pointing" to the same direction as the object in the image.
            </p>

            <p>
                <b>2D location (with initializations provided by a pretrained model).</b> 2D location specifies the location of the center of the object.
            </p>

            <p>
                <b>Distance (with initializations provided by a pretrained model).</b> Distance between the object and the camera. To annotate the distance, make sure the "size" of the rendered object is roughly the same as the "size" of the object in the image.
            </p>

            <p>
                <b>Object quality.</b> Object quality specifies how clearly the object is visible from the image. A "good" object would be clearly visible and not occluded. A "bad" object may be barely visible (imaging drving in heavy fog when cars in front of you are barely visible) or occluded by other objects. Several choices are considered:
                <ul>
                    <li><i>Good.</i> Most part (more than 90%) of the object is clearly visible in the image.</li>
                    <li><i>Partially visible.</i> A small part of the object is occluded by other objects or outside the image (truncated by image boundary).</li>
                    <li><i>Barely visible.</i> only a small part of the object is clearly visible -- the other parts are either occluded or outside the image, or barely visible due to other reasons (e.g., weather).</li>
                    <li><i>Bad quality / no object.</i> Most part of the object is occluded or outside the image; or the pose of the object is very hard to tell.</li>
                </ul>
            </p>

            <p>
                <b>Dense scene.</b> This parameter tells if an object is very close to another object from the same category. Here "close" is defined in the 2D image plane -- two objects are close if the distance between them is small in the 2D image plane.
                <ul>
                    <li><i>Not dense scene.</i> The object is not close to another object from the same category. There can be multiple objects from the same category in one image but the objects are far away from each other.
                    <li><i>Dense scene.</i> The object is very close to another object from the same category. They may occlude each other or simply very close to each other.</li>
                </ul>
            </p>

            <h3 class="pb-2 pt-4">Examples</h3>

            <p>See <a href="https://drive.google.com/file/d/1BiQ4CoYbhABI5S2oC0M7IGqqvUmosnmu/view?usp=sharing">tutorial</a>.</p>

            <h2 class="pb-2">References</h2>

            <p id="ref1">
                [1] <a href="https://arxiv.org/abs/2306.08103">Adding 3D Geometry Control to Diffusion Models</a>
            </p>

            <p id="ref2">
                [2] <a href="https://arxiv.org/abs/2209.05624">Robust Category-Level 6D Pose Estimation with Coarse-to-Fine Rendering of Neural Features</a>
            </p>

        </div>

    </div>
{% endblock %}
